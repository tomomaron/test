# 強化学習で迷路問題

# 目的

このプログラミング課題では、「学習とは？」というところの理解を助けるための導入として，エージェントに迷路問題を Q-Learning で強化学習させましょう．

# やること

## 課題

図１，図２のマップをエージェントに Q-Learning で強化学習させて下さい．ただし，Ｓはエージェントのスタート地点，Ｇはエージェントのゴール地点とします．また，エージェントの「状態」は，マップの絶対座標とします．

| 図 1                                                                                                         | 図 2                                                                                                         |
| ------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------ |
| ![図1](https://user-images.githubusercontent.com/20969270/55776811-e3a22180-5ad8-11e9-907a-8aa620744403.png) | ![図2](https://user-images.githubusercontent.com/20969270/55776815-e735a880-5ad8-11e9-98db-703390d0b149.png) |

なお，Q 値の更新式は次の通りとします．

![Q-Learning](https://latex.codecogs.com/gif.latex?Q%28s%2Ca%29%5Cleftarrow%20Q%28s%2Ca%29+%5Calpha%5Br+%5Cgamma%5Cmax_%7B%5Calpha%27%5Cin%20A%28s%27%29%7DQ%28s%27%2Ca%27%29-Q%28s%2Ca%29%5D)

ただし，Q(s,a)は状態 s で行動 a を取るときの価値，Q(s’,a’)は遷移先での価値，r は状態 s→s’時に得られる報酬，A(s’)は状態 s’で実行可能な行動全体の集合を表します．また学習率 α(0<α≦1)，割引率 γ(0<γ≦1)は定数です．パラメータ参考値として α=0.1，γ=0.9 を提示しておきます．  
また，行動選択には ε-greedy 選択を用いて下さい．こちらも参考値として ε=0.1 を提示しておきます．

## 応用課題（興味がある人はやってみましょう）

状態をエージェントの周囲 8 近傍の部分マップにしてみましょう．学習が`Parameters.ActionChoiceStrategy.GREEDY`ではうまく進まず，`Parameters.ActionChoiceStrategy.E_GREEDY`ではうまく進みます．
その理由についても考察をしてみましょう．  
また，迷路問題を Profit Sharing で学習させ，行動選択にルーレット選択を採用してみましょう．果たして学習できるでしょうか？  
また，強化学習を別の問題にも適用してみましょう．

# ヒント

Q-Learning の pseudo-code を次に示します．参考にして下さい．

```
procedure Q-Learning
begin
    Q(s,a)を初期化，∀s∈S, ∀a∈A;
    初期状態s_0と最終状態s_nの設定;
    for cycle := 1 to MAX_CYCLE do
        s := s_0;
        while s != s_n do
            行動a := ActionSelect(Q,s);
            行動aを実行，報酬rを獲得後，次状態s'へ遷移;
            Q値を更新;
            s := s';
        end
    end
end
```
